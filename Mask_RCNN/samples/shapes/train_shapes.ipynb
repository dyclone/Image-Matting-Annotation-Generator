{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcsab\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dcsab\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dcsab\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dcsab\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dcsab\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dcsab\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dcsab\\anaconda3\\envs\\tf15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1154: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\dcsab\\anaconda3\\envs\\tf15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1188: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\dcsab\\anaconda3\\envs\\tf15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0\n",
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: C:\\Users\\dcsab\\Mask_RCNN\\logs\\shapes20201005T1412\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcsab\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf15]",
   "language": "python",
   "name": "conda-env-tf15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
